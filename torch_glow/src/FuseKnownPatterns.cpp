/**
 * Copyright (c) Glow Contributors. See CONTRIBUTORS file.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include "FuseKnownPatterns.h"

#include <glog/logging.h>

#include <torch/csrc/jit/custom_operator.h>
#include <torch/csrc/jit/passes/alias_analysis.h>
#include <torch/csrc/jit/passes/common_subexpression_elimination.h>
#include <torch/csrc/jit/passes/dead_code_elimination.h>
#include <torch/csrc/jit/passes/subgraph_rewrite.h>
#include <torch/csrc/jit/passes/utils/subgraph_utils.h>

namespace glow {
namespace {
/// This pass fuse the quantized::conv_prepack + quantized::conv2d generated by
/// JIT back to quantized::unpacked_conv2d since we dont have
/// quantized::conv_prepack in glow. However regular packed conv's
/// implementation in glow is still needed.
void fuseConvPrepack(std::shared_ptr<torch::jit::Graph> &graph) {
  std::string convPrepackPattern = R"IR(
graph(%input, %w, %b, %4, %5, %6, %7, %8, %9, %10, %11, %12):
  %prepacked_weight = quantized::conv_prepack(%w, %b, %4, %5, %6, %7)
  %res = quantized::conv2d(%input, %prepacked_weight, %8, %9, %10, %7, %11, %12)
  return (%res))IR";

  std::string convFused = R"IR(
graph(%input, %w, %b, %4, %5, %6, %7, %8, %9, %10, %11, %12):
  %res = glow::unpacked_quantized_conv2d(%input, %w, %b, %8, %9, %10, %7, %11, %12)
  return (%res))IR";

  // Replace conv_prepack + conv2d to unpacked_quantized_conv2d
  torch::jit::SubgraphRewriter convToUnpackedConv;
  convToUnpackedConv.RegisterRewritePattern(convPrepackPattern, convFused);
  convToUnpackedConv.runOnGraph(graph);
}

void fuseLinearPrepack(std::shared_ptr<torch::jit::Graph> &graph) {
  std::string beforePattern = R"IR(
graph(%input, %weights, %bias, %scale, %zero_point):
  %packed_params = quantized::linear_prepack(%weights, %bias)
  %res = quantized::linear(%input, %packed_params, %scale, %zero_point)
  return (%res))IR";

  std::string afterPattern = R"IR(
graph(%input, %weights, %bias, %scale, %zero_point):
  %res = glow::unpacked_quantized_linear(%input, %weights, %bias, %scale, %zero_point)
  return (%res))IR";

  // Replace linear_prepack + quantized::linear to
  // glow::unpacked_quantized_linear
  torch::jit::SubgraphRewriter rewriter;
  rewriter.RegisterRewritePattern(beforePattern, afterPattern);
  rewriter.runOnGraph(graph);
}

void fuseNumToTensorToNum(std::shared_ptr<torch::jit::Graph> &graph) {
  std::string originalPat = R"IR(
graph(%input):
  %res1 = prim::NumToTensor(%input)
  %res2 = aten::Int(%res1)
  return (%res2))IR";

  std::string replacementPat = R"IR(
graph(%input):
  return (%input))IR";

  torch::jit::SubgraphRewriter rewriter;
  rewriter.RegisterRewritePattern(originalPat, replacementPat);
  rewriter.runOnGraph(graph);
}

/// Registers an operator with symbol \p opName but with no implementation.
/// Dummy operators can be used by glow-specific fusion passes prior to loading
/// a glow graph in order to eliminate intermediate values that are unnecessary
/// to Glow such as those created by quantization packing nodes.
void registerDummyOperator(const char *opName) {
  auto options = c10::OperatorOptions();
  options.setAliasAnalysis(at::AliasAnalysisKind::PURE_FUNCTION);

  torch::jit::RegisterOperators op({torch::jit::Operator(
      at::Symbol::fromQualString(opName),
      [](const torch::jit::Node *node) -> torch::jit::Operation {
        LOG(FATAL) << "Operator \"" << (*node)
                   << "\" has no implementation and is meant only as a "
                      "placeholder while fusing ops to run with Glow";
      },
      options)});
}
} // namespace

void fuseKnownPatterns(std::shared_ptr<torch::jit::Graph> &graph) {
  // Register dummy nodes used by custom fusers.
  static std::once_flag onceFlag;
  std::call_once(onceFlag, []() {
    registerDummyOperator("glow::unpacked_quantized_linear");
    registerDummyOperator("glow::unpacked_quantized_conv2d");
  });

  fuseConvPrepack(graph);
  fuseLinearPrepack(graph);
  fuseNumToTensorToNum(graph);
}
} // namespace glow
